{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CorpusLoaders, DataDeps, WordTokenizers\n",
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126678"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = IMDB()\n",
    "\n",
    "sents = Array{String,1}()\n",
    "for file in files.filepaths\n",
    "    open(file) do fileio\n",
    "        for sent in split_sentences(read(fileio, String))\n",
    "            push!(sents, sent)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "length(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#                              #\n",
    "#      WordTokenizers.jl       #\n",
    "#                              #\n",
    "################################\n",
    "using WordTokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6.566557 seconds (14.83 M allocations: 783.295 MiB, 12.03% gc time)\n",
      "  6.086671 seconds (14.02 M allocations: 743.694 MiB, 13.42% gc time)\n",
      "  6.116448 seconds (14.02 M allocations: 743.694 MiB, 13.40% gc time)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "#  NLTK Word Tokenize  #\n",
    "########################\n",
    "\n",
    "@time nltk_word_tokenize.(sents)\n",
    "@time nltk_word_tokenize.(sents)\n",
    "@time nltk_word_tokenize.(sents)\n",
    "println()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  743.69 MiB\n",
       "  allocs estimate:  14019441\n",
       "  --------------\n",
       "  minimum time:     6.223 s (12.20% GC)\n",
       "  median time:      6.223 s (12.20% GC)\n",
       "  mean time:        6.223 s (12.20% GC)\n",
       "  maximum time:     6.223 s (12.20% GC)\n",
       "  --------------\n",
       "  samples:          1\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark nltk_word_tokenize.(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6.638738 seconds (45.11 M allocations: 2.907 GiB, 23.30% gc time)\n",
      "  5.374411 seconds (43.04 M allocations: 2.808 GiB, 30.34% gc time)\n",
      "  5.483399 seconds (43.04 M allocations: 2.808 GiB, 31.05% gc time)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "#   Tweet Tokenizerr   #\n",
    "########################\n",
    "\n",
    "@time tweet_tokenize.(sents)\n",
    "@time tweet_tokenize.(sents)\n",
    "@time tweet_tokenize.(sents)\n",
    "println()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  2.81 GiB\n",
       "  allocs estimate:  43040936\n",
       "  --------------\n",
       "  minimum time:     5.286 s (29.07% GC)\n",
       "  median time:      5.286 s (29.07% GC)\n",
       "  mean time:        5.286 s (29.07% GC)\n",
       "  maximum time:     5.286 s (29.07% GC)\n",
       "  --------------\n",
       "  samples:          1\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "@benchmark tweet_tokenize.(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 23.802682 seconds (64.58 M allocations: 4.216 GiB, 10.52% gc time)\n",
      " 23.483375 seconds (63.77 M allocations: 4.177 GiB, 10.04% gc time)\n",
      " 24.013465 seconds (63.77 M allocations: 4.177 GiB, 10.73% gc time)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "#   TokTok tokenizer   #\n",
    "########################\n",
    "\n",
    "@time tokenize.(sents)\n",
    "@time tokenize.(sents)\n",
    "@time tokenize.(sents)\n",
    "println()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  4.18 GiB\n",
       "  allocs estimate:  63768415\n",
       "  --------------\n",
       "  minimum time:     24.109 s (9.67% GC)\n",
       "  median time:      24.109 s (9.67% GC)\n",
       "  mean time:        24.109 s (9.67% GC)\n",
       "  maximum time:     24.109 s (9.67% GC)\n",
       "  --------------\n",
       "  samples:          1\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark tokenize.(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3.084606 seconds (13.39 M allocations: 503.096 MiB, 28.98% gc time)\n",
      "  2.374276 seconds (12.96 M allocations: 482.001 MiB, 17.61% gc time)\n",
      "  1.928716 seconds (12.96 M allocations: 482.001 MiB)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "# Reversible Tokenizer #\n",
    "########################\n",
    "\n",
    "@time rev_tokenize.(sents)\n",
    "@time rev_tokenize.(sents)\n",
    "@time rev_tokenize.(sents)\n",
    "println()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  482.00 MiB\n",
       "  allocs estimate:  12955494\n",
       "  --------------\n",
       "  minimum time:     1.926 s (0.00% GC)\n",
       "  median time:      2.380 s (20.46% GC)\n",
       "  mean time:        2.260 s (15.21% GC)\n",
       "  maximum time:     2.474 s (22.01% GC)\n",
       "  --------------\n",
       "  samples:          3\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark rev_tokenize.(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <spacy.tokenizer.Tokenizer object at 0x7f5d954942a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################\n",
    "#                              #\n",
    "#            SpaCy             #\n",
    "#                              #\n",
    "################################\n",
    "\n",
    "using PyCall\n",
    "en = pyimport(\"spacy.lang.en\")\n",
    "nlp = en.English()\n",
    "spacy_tokenizer = nlp.Defaults.create_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 27.217917 seconds (1.08 M allocations: 35.951 MiB, 0.06% gc time)\n",
      " 23.444356 seconds (886.25 k allocations: 26.087 MiB, 0.13% gc time)\n",
      " 22.038129 seconds (886.25 k allocations: 26.087 MiB, 0.12% gc time)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@time spacy_tokenizer.(sents)\n",
    "@time spacy_tokenizer.(sents)\n",
    "@time spacy_tokenizer.(sents)\n",
    "println()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  26.09 MiB\n",
       "  allocs estimate:  886243\n",
       "  --------------\n",
       "  minimum time:     21.979 s (0.08% GC)\n",
       "  median time:      21.979 s (0.08% GC)\n",
       "  mean time:        21.979 s (0.08% GC)\n",
       "  maximum time:     21.979 s (0.08% GC)\n",
       "  --------------\n",
       "  samples:          1\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark spacy_tokenizer.(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <function word_tokenize at 0x7f5d93a6e1e0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################\n",
    "#                              #\n",
    "#             NLTK             #\n",
    "#                              #\n",
    "################################\n",
    "\n",
    "using PyCall\n",
    "nltk_tok = pyimport(\"nltk.tokenize\")\n",
    "nltk_tokenizer = nltk_tok.word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32.385245 seconds (17.41 M allocations: 485.114 MiB, 2.10% gc time)\n",
      " 32.642742 seconds (16.76 M allocations: 451.949 MiB, 2.34% gc time)\n",
      " 32.291003 seconds (16.76 M allocations: 451.949 MiB)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@time nltk_tokenizer.(sents)\n",
    "@time nltk_tokenizer.(sents)\n",
    "@time nltk_tokenizer.(sents)\n",
    "println()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  451.95 MiB\n",
       "  allocs estimate:  16761585\n",
       "  --------------\n",
       "  minimum time:     34.053 s (1.35% GC)\n",
       "  median time:      34.053 s (1.35% GC)\n",
       "  mean time:        34.053 s (1.35% GC)\n",
       "  maximum time:     34.053 s (1.35% GC)\n",
       "  --------------\n",
       "  samples:          1\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark nltk_tokenizer.(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
